//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35404655
// Cuda compilation tools, release 12.8, V12.8.61
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	_Z14fmaRnF32Kerneli

.visible .entry _Z14fmaRnF32Kerneli(
	.param .u32 _Z14fmaRnF32Kerneli_param_0
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<11>;


	ld.param.u32 	%r7, [_Z14fmaRnF32Kerneli_param_0];
	// begin inline asm
	.reg .f32 %fa<8>, %fb<8>, %fc<8>, %fd<8>;
	mov.f32 %fa0, 1.23;
	mov.f32 %fa1, 2.34;
	mov.f32 %fa2, 3.45;
	mov.f32 %fa3, 4.56;
	mov.f32 %fa4, 5.67;
	mov.f32 %fa5, 6.78;
	mov.f32 %fa6, 7.89;
	mov.f32 %fa7, 8.90;
	mov.f32 %fb0, 0.01;
	mov.f32 %fb1, 0.12;
	mov.f32 %fb2, 0.23;
	mov.f32 %fb3, 0.34;
	mov.f32 %fb4, 0.45;
	mov.f32 %fb5, 0.56;
	mov.f32 %fb6, 0.67;
	mov.f32 %fb7, 0.78;
	mov.f32 %fc0, 0.11;
	mov.f32 %fc1, 0.22;
	mov.f32 %fc2, 0.33;
	mov.f32 %fc3, 0.44;
	mov.f32 %fc4, 0.55;
	mov.f32 %fc5, 0.66;
	mov.f32 %fc6, 0.77;
	mov.f32 %fc7, 0.88;
	
	// end inline asm
	setp.lt.s32 	%p1, %r7, 1;
	@%p1 bra 	$L__BB0_6;

	add.s32 	%r8, %r7, -1;
	and.b32  	%r10, %r7, 3;
	setp.lt.u32 	%p2, %r8, 3;
	@%p2 bra 	$L__BB0_4;

	sub.s32 	%r9, %r7, %r10;

$L__BB0_3:
	// begin inline asm
	fma.rn.f32 %fd0, %fa0, %fb0, %fc0;
	fma.rn.f32 %fd1, %fa1, %fb1, %fc1;
	fma.rn.f32 %fd2, %fa2, %fb2, %fc2;
	fma.rn.f32 %fd3, %fa3, %fb3, %fc3;
	fma.rn.f32 %fd4, %fa4, %fb4, %fc4;
	fma.rn.f32 %fd5, %fa5, %fb5, %fc5;
	fma.rn.f32 %fd6, %fa6, %fb6, %fc6;
	fma.rn.f32 %fd7, %fa7, %fb7, %fc7;
	
	// end inline asm
	// begin inline asm
	fma.rn.f32 %fd0, %fa0, %fb0, %fc0;
	fma.rn.f32 %fd1, %fa1, %fb1, %fc1;
	fma.rn.f32 %fd2, %fa2, %fb2, %fc2;
	fma.rn.f32 %fd3, %fa3, %fb3, %fc3;
	fma.rn.f32 %fd4, %fa4, %fb4, %fc4;
	fma.rn.f32 %fd5, %fa5, %fb5, %fc5;
	fma.rn.f32 %fd6, %fa6, %fb6, %fc6;
	fma.rn.f32 %fd7, %fa7, %fb7, %fc7;
	
	// end inline asm
	// begin inline asm
	fma.rn.f32 %fd0, %fa0, %fb0, %fc0;
	fma.rn.f32 %fd1, %fa1, %fb1, %fc1;
	fma.rn.f32 %fd2, %fa2, %fb2, %fc2;
	fma.rn.f32 %fd3, %fa3, %fb3, %fc3;
	fma.rn.f32 %fd4, %fa4, %fb4, %fc4;
	fma.rn.f32 %fd5, %fa5, %fb5, %fc5;
	fma.rn.f32 %fd6, %fa6, %fb6, %fc6;
	fma.rn.f32 %fd7, %fa7, %fb7, %fc7;
	
	// end inline asm
	// begin inline asm
	fma.rn.f32 %fd0, %fa0, %fb0, %fc0;
	fma.rn.f32 %fd1, %fa1, %fb1, %fc1;
	fma.rn.f32 %fd2, %fa2, %fb2, %fc2;
	fma.rn.f32 %fd3, %fa3, %fb3, %fc3;
	fma.rn.f32 %fd4, %fa4, %fb4, %fc4;
	fma.rn.f32 %fd5, %fa5, %fb5, %fc5;
	fma.rn.f32 %fd6, %fa6, %fb6, %fc6;
	fma.rn.f32 %fd7, %fa7, %fb7, %fc7;
	
	// end inline asm
	add.s32 	%r9, %r9, -4;
	setp.ne.s32 	%p3, %r9, 0;
	@%p3 bra 	$L__BB0_3;

$L__BB0_4:
	setp.eq.s32 	%p4, %r10, 0;
	@%p4 bra 	$L__BB0_6;

$L__BB0_5:
	.pragma "nounroll";
	// begin inline asm
	fma.rn.f32 %fd0, %fa0, %fb0, %fc0;
	fma.rn.f32 %fd1, %fa1, %fb1, %fc1;
	fma.rn.f32 %fd2, %fa2, %fb2, %fc2;
	fma.rn.f32 %fd3, %fa3, %fb3, %fc3;
	fma.rn.f32 %fd4, %fa4, %fb4, %fc4;
	fma.rn.f32 %fd5, %fa5, %fb5, %fc5;
	fma.rn.f32 %fd6, %fa6, %fb6, %fc6;
	fma.rn.f32 %fd7, %fa7, %fb7, %fc7;
	
	// end inline asm
	add.s32 	%r10, %r10, -1;
	setp.ne.s32 	%p5, %r10, 0;
	@%p5 bra 	$L__BB0_5;

$L__BB0_6:
	ret;

}

